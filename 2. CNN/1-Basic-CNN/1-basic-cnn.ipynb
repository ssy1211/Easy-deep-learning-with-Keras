{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Convolutional Neural Networks (CNN)\n",
    "- Objective: create basic CNN models with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://adeshpande3.github.io/assets/Cover.png\" style=\"width: 800px\"/>\n",
    "\n",
    "<br>\n",
    "- Fundamental CNN structures: CNNs are similar to MLPs since they only feed signals forward (feedforward nets), but have different kind of layers unique to CNNs\n",
    "    - ** Convolutional layer** : process data in a small receptive field (i.e., filter)\n",
    "    - ** Pooling layer** : downsample along 2 dimensions (usually width and height) \n",
    "    - ** Dense (fully connected) layer** : similar to hidden layers of MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cs231n.github.io/assets/cnn/convnet.jpeg\" style=\"width: 600px\"/>\n",
    "<br>\n",
    "<center> **ConvNet architecture** </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datset\n",
    "- Digits dataset in sklearn\n",
    "- Doc: http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  1\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(data.images[1])    # show first number in the dataset\n",
    "plt.show()\n",
    "print('label: ', data.target[1])    # label = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = data.images\n",
    "y_data = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 8, 8)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "# shape of data\n",
    "print(X_data.shape)    # (8 X 8) format\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X_data into 3-D format\n",
    "# note that this follows image format of Tensorflow backend\n",
    "X_data = X_data.reshape((X_data.shape[0], X_data.shape[1], X_data.shape[2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of y_data\n",
    "y_data = to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition data into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.3, random_state = 777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1257, 8, 8, 1)\n",
      "(540, 8, 8, 1)\n",
      "(1257, 10)\n",
      "(540, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating model\n",
    "- Creating a model is same with MLP (Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convolutional Layer\n",
    "- In general, 2D convolutional layer is used for image processing\n",
    "    - Size of filter (designated by 'kernel_size' parameter) defines **width and height of receptive field **\n",
    "    - Number of filters (designated by 'filters' parameter) is equal to **depth of the next layer**\n",
    "    - Strides (designated by 'strides' parameter) is** how far a filter makes change in its position** for each move\n",
    "    - Image can be **zero-padded** in order to prevent getting too small (designated by 'padding' parameter)\n",
    "- Doc: https://keras.io/layers/convolutional/\n",
    "<img src=\"http://cs231n.github.io/assets/cnn/depthcol.jpeg\" style=\"width: 400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution layer\n",
    "model.add(Conv2D(input_shape = (X_data.shape[1], X_data.shape[2], X_data.shape[3]), filters = 10, kernel_size = (3,3), strides = (1,1), padding = 'valid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Activation Layer\n",
    "- Identical to the activation layers in MLP\n",
    "- In general, relu is used as well\n",
    "- Doc: http://cs231n.github.io/assets/cnn/depthcol.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pooling layer\n",
    "- In general, max pooling method is used\n",
    "- Reduces number of parameters\n",
    "- Doc: https://keras.io/layers/pooling/\n",
    "<img src=\"http://cs231n.github.io/assets/cnn/maxpool.jpeg\" style=\"width: 600px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Dense (fully connected layer)\n",
    "- Convolutional & pooling layers can be connected to dense layers\n",
    "- Sometimes, dense layers can be omitted\n",
    "- Doc: https://keras.io/layers/core/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior layer should be flattend to be connected to dense layers\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense layer with 50 neurons\n",
    "model.add(Dense(50, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final layer with 10 neurons to classify the instances\n",
    "model.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model compile & train\n",
    "- Identical to compiling MLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(lr = 0.001)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size = 50, validation_split = 0.2, epochs = 100, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-1a74c43e2cab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#plt.plot(history.history['acc'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'upper left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_acc'"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['training', 'validation'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540/540 [==============================] - 0s 52us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9777777791023254\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even simple CNN model shows fine performance of **97% **test accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
